{"cells":[{"cell_type":"markdown","metadata":{"id":"xpPgdpjtuYuu"},"source":["# **CS-GY-6613 AI Final Project: Separating Perception and Reasoning via Relation Networks**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LhR6mxbg2Zv"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJrDhYw5rJ5u"},"outputs":[],"source":["\"\"\"Generate image feature maps using CNN\"\"\"\n","class ConvBlock(nn.Module):\n","    def __init__(self):\n","        super(ConvBlock, self).__init__()\n","        # CONV input dimention (64,3,75,75) for 64 batch size, Sort-of-CLEVR images size 75 Ã— 75 x 3, we swap the axes to make it to 3 x 75 x 75\n","\n","        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)       # filter input = 3, number of filters = 32 kernels, kernel size = 3\n","        self.batch_norm1 = nn.BatchNorm2d(32)                       # 2d Batch Normalization with num_features 32\n","                                                                    # CONV1 output dimention (64,32,38,38)                     \n","\n","        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)      # filter input = 3, number of filters = 64 kernels, kernel size = 3\n","        self.batch_norm2 = nn.BatchNorm2d(64)                       # 2d Batch Normalization with num_features 64\n","                                                                    # CONV2 output dimention (64,64,19,19)\n","\n","        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)     # filter input = 3, number of filters = 128 kernels, kernel size = 3\n","        self.batch_norm3 = nn.BatchNorm2d(128)                      # 2d Batch Normalization with num_features128\n","                                                                    # CONV3 output dimention (64,128,10,10)\n","\n","        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)     # filter input = 3, number of filters = 256 kernels, kernel size = 3\n","        self.batch_norm4 = nn.BatchNorm2d(256)                      # 2d Batch Normalization with num_features 256\n","                                                                    # final Conv4 output feature dimention (64,256,5,5)\n","        \n","\n","    def forward(self, img):\n","        # using ReLU activation and Batch Norm after at CONV layer\n","        x = self.conv1(img)              # img input dimention (64,3,75,75)\n","        x = F.relu(x)\n","        x = self.batch_norm1(x)\n","        \n","        x = self.conv2(x)                # input dimention (64,32,38,38)\n","        x = F.relu(x)\n","        x = self.batch_norm2(x)\n","        \n","        x = self.conv3(x)                # input dimention (64,64,19,19)\n","        x = F.relu(x)\n","        x = self.batch_norm3(x)\n","        \n","        x = self.conv4(x)                # input dimention (64,128,10,10)\n","        x = F.relu(x)\n","        x = self.batch_norm4(x)\n","        \n","        return x                         # x output feature dimention = (64,256,5,5)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rg7gviN4lU9_"},"outputs":[],"source":["\"\"\"Coordination position information\"\"\"\n","# The coordinate position vector is a vector to provide pixel position information\n","def cvt_coord(i):\n","    return [(i/5-2)/2., (i%5-2)/2.]                # based on the size of the final CNN feature map (5,5), dividing i by 5\n","    \n","def get_coord():\n","    # For preparing the coord tensor, using dim1 = 25 because the size of the final conv_feature_map is [BS=64,256,5,5]\n","    # thus forming a 25 object feature map for each image in the mini-batch\n","    np_coord_tensor = np.zeros((64, 25, 2))\n","    for i in range(25):\n","        np_coord_tensor[:,i,:] = np.array(cvt_coord(i))\n","        \n","    return np_coord_tensor                         # size of the coord tensor (25,25,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqOnEXGglSfb"},"outputs":[],"source":["\"\"\"f_phi classifier\"\"\"\n","class FCBlock(nn.Module):\n","  def __init__(self):\n","    super(FCBlock, self).__init__()\n","    self.fc1 = nn.Linear(2000, 2000)          # FC layer with in_features 2000, out_features 2000 \n","    self.fc2 = nn.Linear(2000, 1000)          # FC layer with in_features 2000, out_features 1000 \n","    self.fc3 = nn.Linear(1000, 500)           # FC layer with in_features 1000, out_features 500 \n","    self.fc4 = nn.Linear(500, 100)            # FC layer with in_features 500, out_features 100 \n","    self.out = nn.Linear(100, 10)             # FC layer with in_features 100, out_features 10, for 10 possible classes (answer vector size) \n","        \n","  def forward(self, x):\n","    # using ReLU activation after each FC layer (except the final output FC layer)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    x = F.relu(x)\n","    x = self.fc4(x)\n","    x = F.relu(x)\n","    x = self.out(x)\n","    # no softmax activation required for the final FC layer, since PyTorch Cross Entropy takes in logits as inputs\n","    \n","    return x                         # x out_features dimention = 10, for 10 possible classes (answer vector size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5XOMP7UNlYdm"},"outputs":[],"source":["class RN(nn.Module):\n","    def __init__(self, batch_size=64):\n","        super(RN, self).__init__()\n","        # Define the parameters for the RN network\n","        self.name = 'RN'\n","\n","        \"\"\"Generate feature maps using CNN\"\"\"\n","        # Conv block to generate CNN feature map for each image\n","        self.conv = ConvBlock()                                  # final Conv output feature dimentions (64,256,5,5)\n","        \n","        \"\"\"g_theta DNN\"\"\"\n","        self.g_fc1 = nn.Linear((256+2)*2+11, 2000)         # g_fc1 input dim = ((num of filters per object + 2 coord.)*2) + question vector\n","        self.g_fc2 = nn.Linear(2000, 2000)                 # FC layer with in_features 2000, out_features 2000 \n","        self.g_fc3 = nn.Linear(2000, 2000)                 # FC layer with in_features 2000, out_features 2000 \n","        self.g_fc4 = nn.Linear(2000, 2000)                 # FC layer with in_features 2000, out_features 2000 \n","        \n","        \"\"\"f_phi classifier\"\"\"\n","        self.fcout = FCBlock()                             # set of 4 FC layers with first in_features 2000, final out_features 10 for 10 classes\n","        \n","        \"\"\"Coordination position information\"\"\"\n","        # prepare coord tensor\n","        self.coord_oi = Variable(torch.FloatTensor(batch_size, 2).cuda()) \n","        self.coord_oj = Variable(torch.FloatTensor(batch_size, 2).cuda())\n","        \n","        # For preparing the coord tensor, using dim1 = 25 because the size of the final conv_feature_map is [BS=64,256,5,5]\n","        # thus forming a 25 object feature map for each image in the mini-batch\n","        self.coord_tensor = Variable(torch.FloatTensor(64, 25, 2).cuda())\n","        self.coord_tensor.data.copy_(torch.from_numpy(get_coord()))\n","               \n","        # using adam optimizer with learning rate 0.0001\n","        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n","            \n","    def forward(self, img, ques):\n","        \"\"\"Generate feature maps for each image\"\"\"\n","        x = self.conv(img)                             # x dimension (64 batch, 256 channels, 5,5) \n","\n","        # define parameters\n","        mb = x.size()[0]                               # instead of using for loops, accessing objects for g_theta in a vectorized manner, mb = 64\n","        n_channels = x.size()[1]                       # num_channels = 256\n","        d = x.size()[2]                                # d = 5\n","\n","        \"\"\"Add coordination position information\"\"\"\n","        # x.view(mb, n_channels, d*d) dimension = (64, 256, 25)\n","        x_flat = x.view(mb, n_channels, d*d).permute(0,2,1)         # x_flat dimension = (64, 25, 256), after permute to change axes\n","        x_flat = torch.cat([x_flat, self.coord_tensor], 2)          # x_flat dimension = (64, 25, 258), add coordinates\n","\n","        \"\"\"Generate object pair with questions\"\"\"\n","        # add question everywhere\n","        # initial ques dimension =  (64 bs, 11 que_size)\n","        ques = torch.unsqueeze(ques, 1)                 # ques dimension = (64,1,11), adding another ax at loc = 1\n","        ques = ques.repeat(1, 25, 1)                    # ques dimension = (64,25,11), repeating ques across dim 1\n","        ques = torch.unsqueeze(ques, 2)                 # ques dimension = (64,25,1,11), adding another ax at loc = 2\n","        \n","        # cast all possible pairs of the image feature maps against each other, concatenate the question features to them\n","        # x_flat dimension = (64, 25, 258)\n","        x_i = torch.unsqueeze(x_flat, 1)                # x_i dimension = (64,1,25,258), adding another ax to x_flat at loc = 1\n","        x_i = x_i.repeat(1, 25, 1, 1)                   # x_i dimension = (64,25,25,258), copying / repeating pixels across rows (dim 1)\n","\n","        x_j = torch.unsqueeze(x_flat, 2)                # x_j dimension = (64,25,1,258), adding another ax to x_flat at loc = 2\n","        x_j = torch.cat([x_j, ques], 3)                 # x_j dimension = (64,25,1,269), adding ques of size 11 along ax=3 => dim3_size=258+11=269\n","        x_j = x_j.repeat(1, 1, 25, 1)                   # x_j dimension = (64,25,25,269), copying / repeating pixels across columns (dim 2)\n","        \n","        # concatenate all together\n","        x_full = torch.cat([x_i, x_j], 3)               # x_full dimension = (64,25,25,527), concat x_i, x_j along ax=3 => dim3_size=(2*258)+11=527\n","        \n","        # reshape for passing through g_theta network\n","        x_ = x_full.view(mb * (d*d) * (d*d), 527)       # x_ dimension = (64*25*25, 527) = (40000, 527)\n","            \n","        \"\"\"g_theta\"\"\"\n","        # pass through 4 layers MLP for function g_theta, use ReLU activation after each fc layer\n","        x_ = self.g_fc1(x_)\n","        x_ = F.relu(x_)\n","        x_ = self.g_fc2(x_)\n","        x_ = F.relu(x_)\n","        x_ = self.g_fc3(x_)\n","        x_ = F.relu(x_)\n","        x_ = self.g_fc4(x_)\n","        x_ = F.relu(x_)\n","        \n","        # reshape and sum for passing through f_phi network\n","        x_g = x_.view(mb, (d*d) * (d*d), 2000)           # x_g dimension = (64,25*25,2000) = (64,625,2000), dim3=2000 since next f_fc1 is of size 2000\n","        # the first dimension was inflated 625 times more than the original batch size when we created new object pairs\n","        # hence we compute the element-wise sum to scale back to the original batch size\n","        x_g = x_g.sum(1).squeeze()                       # x_g dimension = (64,1,2000), after element wise sum across ax=1\n","        \n","\n","        \"\"\"f_phi\"\"\"\n","        # pass through the four layers MLP for function f_phi and return the class probabilities predictions\n","        return self.fcout(x_g)\n","        \n","    def train_(self, img, ques, label):\n","        self.optimizer.zero_grad()\n","        output = self(img, ques)\n","        loss = F.cross_entropy(output, label)                 # using cross entropy loss\n","        loss.backward()\n","        pred = output.data.max(1)[1]                          # predicting class probabilities\n","        correct = pred.eq(label.data).cpu().sum()             \n","        accuracy = correct * 100. / len(label)                # calculating accuracy\n","        return accuracy, loss\n","        \n","    def test_(self, img, ques, label):\n","        output = self(img, ques)\n","        loss = F.cross_entropy(output, label)                 # using cross entropy loss\n","        pred = output.data.max(1)[1]                          # predicting class probabilities\n","        correct = pred.eq(label.data).cpu().sum()\n","        accuracy = correct * 100. / len(label)                # calculating accuracy\n","        return accuracy, loss\n","        \n","    def save_model(self, epoch):\n","        torch.save(self.state_dict(), f\"{self.name}_{epoch}.pth\")      # saving the model using torch.save"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1667757823754,"user":{"displayName":"Atharva Bhagwat","userId":"17933411250602058523"},"user_tz":300},"id":"Hr_h4nRoEpEw","outputId":"e7c9d831-bfbb-4278-eca7-f911de2347d0"},"outputs":[{"data":{"text/plain":["RN(\n","  (conv): ConvBlock(\n","    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2): Conv2d(24, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv3): Conv2d(24, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (batch_norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv4): Conv2d(24, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","    (batch_norm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (g_fc1): Linear(in_features=527, out_features=2000, bias=True)\n","  (g_fc2): Linear(in_features=2000, out_features=2000, bias=True)\n","  (g_fc3): Linear(in_features=2000, out_features=2000, bias=True)\n","  (g_fc4): Linear(in_features=2000, out_features=2000, bias=True)\n","  (fcout): FCBlock(\n","    (fc1): Linear(in_features=2000, out_features=2000, bias=True)\n","    (fc2): Linear(in_features=2000, out_features=1000, bias=True)\n","    (fc3): Linear(in_features=1000, out_features=500, bias=True)\n","    (fc4): Linear(in_features=500, out_features=100, bias=True)\n","    (out): Linear(in_features=100, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model = RN()\n","model"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
